# Dockerfile.vertex
# Custom container for Vertex AI training jobs (EfficientNet / ResNet).
#
# Build and push to Artifact Registry:
#   gcloud builds submit \
#     --config cloudbuild.yaml .
#
# ─────────────────────────────────────────────────────────────────────
# WHY THIS FILE LOOKS THE WAY IT DOES
#
# 1. We start from the official TF GPU image so TensorFlow is already
#    compiled with CUDA support.  Re-installing TF from PyPI would
#    silently swap the GPU build for a CPU-only wheel — the single
#    most common Vertex AI GPU pitfall.
#
# 2. TF 2.19.1 was compiled against CUDA 12.5 / cuDNN 9, but the base
#    image ships CUDA 12.3 / cuDNN 8.  The explicit nvidia-* pip
#    packages bridge that gap at the Python level.
#
# 3. The NVIDIA_VISIBLE_DEVICES / NVIDIA_DRIVER_CAPABILITIES env vars
#    are required for Vertex AI's container runtime to expose GPUs.
#    Without them the GPU is invisible even though the host has one.
#
# 4. The build-time assertion (`RUN python -c "..."`) catches GPU
#    setup problems at `docker build` time rather than after you've
#    submitted a job.
# ─────────────────────────────────────────────────────────────────────

FROM tensorflow/tensorflow:2.19.0-gpu

WORKDIR /app

# ── GPU detection env vars (required for Vertex AI) ──────────────────
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility

# ── Bridge CUDA 12.3 → 12.5 / cuDNN 8 → 9 for TF 2.19.1 ───────────
RUN pip install --no-cache-dir --no-deps \
    nvidia-cublas-cu12==12.5.3.2 \
    nvidia-cuda-cupti-cu12==12.5.82 \
    nvidia-cuda-nvrtc-cu12==12.5.82 \
    nvidia-cuda-runtime-cu12==12.5.82 \
    nvidia-cudnn-cu12==9.3.0.75 \
    nvidia-cufft-cu12==11.2.3.61 \
    nvidia-curand-cu12==10.3.6.82 \
    nvidia-cusolver-cu12==11.6.3.83 \
    nvidia-cusparse-cu12==12.5.1.3 \
    nvidia-nccl-cu12==2.25.1 \
    nvidia-nvjitlink-cu12==12.5.82

# ── Install uv (fast Python package manager) ────────────────────────
COPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv

# ── Install project dependencies ────────────────────────────────────
COPY pyproject.toml uv.lock README.md /app/

# Export pinned deps with vertex extras, FILTER OUT tensorflow/keras/numpy/nvidia
# to keep the GPU build intact, then install with --no-deps to prevent
# transitive deps from pulling CPU-only TF back in.
RUN uv export --frozen --no-dev --no-hashes \
      --extra vertex -o /tmp/requirements.txt && \
    grep -vE "^(tensorflow|keras|tf-keras|numpy|nvidia-)==" \
      /tmp/requirements.txt > /tmp/reqs_filtered.txt && \
    uv pip install --system --no-deps -r /tmp/reqs_filtered.txt

# ── Upgrade Keras to match local version ──────────────────────────────
# Checkpoints saved with Keras 3.9+ include quantization_config in
# layer configs.  The TF 2.19.0 base image ships Keras ~3.8 which
# cannot deserialize those checkpoints.
RUN pip install --no-cache-dir keras==3.13.2

# ── Build-time GPU sanity check ─────────────────────────────────────
# Fails `docker build` immediately if GPU libraries are broken.
RUN python -c "\
import tensorflow as tf; \
assert tf.test.is_built_with_cuda(), \
    'FATAL: TF not built with CUDA'; \
print(f'OK: TF {tf.__version__}, CUDA=True')"

# ── Copy project source code ────────────────────────────────────────
COPY src/ /app/src/
COPY scripts/ /app/scripts/
COPY configs/ /app/configs/

# ── Python path ──────────────────────────────────────────────────────
ENV PYTHONPATH=/app/src

# Default command (overridden by Vertex AI job spec)
CMD ["python", "--version"]
